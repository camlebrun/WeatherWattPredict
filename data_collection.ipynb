{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Collection\n",
    "\n",
    "Due to limitations in the [opendatasoft](https://public.opendatasoft.com/explore/dataset/donnees-synop-essentielles-omm/table/?flg=fr-fr&sort=date) API for `Observation météorologique historiques France (SYNOP)` and the presence of approximately 75% missing data in the dataset (upon downloading from the website), certain challenges are encountered.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In devising our data collection strategy, we are presented with two viable options:\n",
    "\n",
    "1. Opt for a less granular approach by utilizing main station data.\n",
    "2. Pursue a more granular path by incorporating data from all stations.\n",
    "\n",
    "Nevertheless, the latter option demands a substantial investment of time, given the necessity to manually download individual files for each department and subsequently integrate them. After careful consideration, I advocate for the first option. This choice not only mitigates time constraints but also aligns with best practices in computational efficiency within the context of our data science workflow.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Le fichier CSV fusionné a été créé : ./data/data_departements.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "import pandas as pd\n",
    "from io import BytesIO\n",
    "import gzip\n",
    "import logging\n",
    "\n",
    "# Définir le répertoire de destination\n",
    "destination_directory = \"./data/\"  # Remplacez cela par le chemin de votre choix\n",
    "\n",
    "# Vérifier si le répertoire de destination existe, sinon le créer\n",
    "if not os.path.exists(destination_directory):\n",
    "    os.makedirs(destination_directory)\n",
    "\n",
    "url_template = \"https://object.files.data.gouv.fr/meteofrance/data/synchro_ftp/BASE/QUOT/Q_{departement}_previous-1950-2021_RR-T-Vent.csv.gz\"\n",
    "\n",
    "# Créer une liste vide pour stocker les DataFrames\n",
    "merged_df = pd.DataFrame()\n",
    "\n",
    "# Créer un handler pour les logs en live\n",
    "handler = logging.StreamHandler()\n",
    "handler.setLevel(logging.INFO)\n",
    "\n",
    "# Définir le format du message de journal du handler\n",
    "formatter = logging.Formatter(\"%(asctime)s %(levelname)s: %(message)s\")\n",
    "handler.setFormatter(formatter)\n",
    "\n",
    "# Ajouter le handler au journal\n",
    "log = logging.getLogger(__name__)\n",
    "log.addHandler(handler)\n",
    "\n",
    "for departement_number in range(1, 4):\n",
    "    # Ignorer le département 62\n",
    "    if departement_number != 62:\n",
    "        # Formater l'URL avec le numéro du département\n",
    "        url = url_template.format(departement=str(departement_number).zfill(2))\n",
    "\n",
    "        # Réaliser la requête HTTP pour télécharger le fichier\n",
    "        response = requests.get(url, stream=True)\n",
    "\n",
    "        # Vérifier si la requête a réussi (code 200)\n",
    "        if response.status_code == 200:\n",
    "            # Utiliser BytesIO pour traiter les données en mémoire\n",
    "            with BytesIO(response.content) as compressed_file:\n",
    "                # Utiliser gzip pour décompresser les données\n",
    "                with gzip.open(compressed_file, 'rt') as csv_file:\n",
    "                    try:\n",
    "                        # Lire le CSV dans un DataFrame pandas\n",
    "                        df = pd.read_csv(csv_file)\n",
    "\n",
    "                        # Fusionner le DataFrame au DataFrame existant\n",
    "                        merged_df = pd.concat([merged_df, df], ignore_index=True)\n",
    "\n",
    "                        # Enregistrer le message de journal\n",
    "                        log.info(f\"Le fichier CSV a été fusionné : {departement_number}\")\n",
    "\n",
    "\n",
    "                    except pd.errors.ParserError as e:\n",
    "                        log.error(f\"Erreur lors de la lecture du fichier CSV pour le département {departement_number}: {e}\")\n",
    "        else:\n",
    "            log.error(f\"Échec du téléchargement pour le département {departement_number}. Code de statut HTTP : {response.status_code}\")\n",
    "\n",
    "# Définir le chemin du fichier de sortie dans le répertoire de destination\n",
    "output_file_path = os.path.join(destination_directory, \"data_departements.csv\")\n",
    "\n",
    "# Écrire les données fusionnées dans le fichier CSV de sortie\n",
    "merged_df.to_csv(output_file_path, index=False)\n",
    "\n",
    "print(f\"Le fichier CSV fusionné a été créé : {output_file_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "elec_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
